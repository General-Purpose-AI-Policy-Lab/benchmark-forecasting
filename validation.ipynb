{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50be6f0b",
   "metadata": {},
   "source": [
    "This notebook performs the following validations on the forecasting models:\n",
    "1. Calibration\n",
    "2. Comparison Harvey vs logistic\n",
    "3. Comparison independant vs joint model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba619ff",
   "metadata": {},
   "source": [
    "# Imports and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import arviz as az\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.stats import energy_distance\n",
    "from typing import Literal\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b6acc",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c676a3b",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "    path: str = \"benchmark_data_processed/all_normalized_updated_benchmarks.csv\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Load the benchmark dataset from a formatted CSV file and returns it as a pandas DataFrame with appropriate data types.\"\"\"\n",
    "    dataset = (\n",
    "        pd.read_csv(path)\n",
    "        .astype(\n",
    "            {\n",
    "                \"benchmark\": \"string\",\n",
    "                \"release_date\": \"datetime64[ns]\",\n",
    "                \"score\": \"float64\",\n",
    "                \"lower_bound\": \"float64\",\n",
    "            }\n",
    "        )\n",
    "        .dropna(subset=[\"benchmark\", \"release_date\", \"score\", \"lower_bound\"])\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def format_dataset_for_modeling(\n",
    "    dataset: pd.DataFrame,\n",
    "    top_n: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Prepare the dataset for modeling by filtering to the top_n frontier scores and adding necessary columns.\n",
    "\n",
    "    Args:\n",
    "        dataset: A dataset containing benchmark data.\n",
    "        top_n: Number of top scores to consider when fitting the model.\n",
    "    Returns:\n",
    "        A formatted dataset ready for modeling.\n",
    "    \"\"\"\n",
    "    dataset = _get_frontier(dataset, top_n=top_n)\n",
    "    dataset = dataset.assign(\n",
    "        days=lambda df: (\n",
    "            df[\"release_date\"]\n",
    "            - df.groupby(\"benchmark\")[\"release_date\"].transform(\"min\")\n",
    "        ).dt.days\n",
    "    ).assign(\n",
    "        # Midpoint of the time range in days is simple the max divided by two since min is zero\n",
    "        days_mid=lambda df: (df.groupby(\"benchmark\")[\"days\"].transform(\"max\") / 2.0)\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def fit_model(\n",
    "    dataset: pd.DataFrame,\n",
    "    sigmoid_kind: Literal[\"logistic\", \"harvey\"] = \"logistic\",\n",
    "    joint: bool = True,\n",
    "    top_n: int = 3,\n",
    "    n_samples: int = 2000,\n",
    "    n_tune: int = 1000,\n",
    "    progressbar: bool = True,\n",
    ") -> tuple[az.InferenceData, pm.Model]:\n",
    "    \"\"\"Fit a Bayesian model to the dataset using the specified sigmoid function.\n",
    "\n",
    "    This allows benchmarks to inform each other through common priors on:\n",
    "    - L_mu, L_sigma: (upper) asymptote distribution parameters\n",
    "    - k_mu, k_sigma: growth rate distribution parameters\n",
    "    - xi_base_mu, xi_base_sigma: noise level distribution parameters\n",
    "    - s_mu, s_sigma: skewness distribution parameters\n",
    "\n",
    "    Args:\n",
    "        dataset: A dataset containing benchmark data. It must be formatted by `format_dataset_for_modeling` first.\n",
    "        sigmoid_kind: Type of sigmoid function to model the latent mean performance growth ('logistic' or 'harvey').\n",
    "        joint: Whether to fit a joint model with shared hyperparameters across benchmarks.\n",
    "        top_n: Number of top scores to consider when fitting the model. If top_n=1, only the frontier scores are used.\n",
    "        n_samples: Number of MCMC samples to draw from the posterior distribution.\n",
    "        n_tune: Number of tuning steps for the MCMC sampler.\n",
    "        progressbar: Whether to display a progress bar during sampling.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing an arviz InferenceData object with the posterior samples and the pymc Model object.\n",
    "    \"\"\"\n",
    "    benchmark_idx, benchmark_names = pd.factorize(dataset[\"benchmark\"], sort=True)\n",
    "    dataset[\"benchmark_idx\"] = benchmark_idx\n",
    "    coords = {\n",
    "        \"benchmark\": benchmark_names,\n",
    "        \"obs\": dataset.index,\n",
    "    }\n",
    "\n",
    "    with pm.Model(coords=coords) as model:\n",
    "        # Upper asymptote\n",
    "        L_min = 0.75\n",
    "        L_max = 1.0\n",
    "        L_range = L_max - L_min\n",
    "        L_raw_mu = pm.Beta(\n",
    "            \"L_raw_mu\",\n",
    "            mu=(0.96 - L_min) / L_range,\n",
    "            sigma=0.02 / L_range,\n",
    "            dims=None if joint else \"benchmark\",\n",
    "        )\n",
    "        L_raw_sigma = pm.HalfNormal(\n",
    "            \"L_raw_sigma\", sigma=0.02 / L_range, dims=None if joint else \"benchmark\"\n",
    "        )\n",
    "        L_raw = pm.Beta(\"L_raw\", mu=L_raw_mu, sigma=L_raw_sigma, dims=\"benchmark\")\n",
    "        L = pm.Deterministic(\"L\", L_min + L_range * L_raw, dims=\"benchmark\")\n",
    "\n",
    "        # Lower bound\n",
    "        l = pm.Data(\n",
    "            \"l\",\n",
    "            dataset[\"lower_bound\"].groupby(dataset[\"benchmark_idx\"]).first().values,\n",
    "            dims=\"benchmark\",\n",
    "        )\n",
    "\n",
    "        # Inflection point\n",
    "        days_mid = dataset[\"days_mid\"].groupby(dataset[\"benchmark_idx\"]).first().values\n",
    "        tau = pm.Gumbel(\"tau\", mu=days_mid, beta=365 * 2, dims=\"benchmark\")\n",
    "\n",
    "        # Times of observations\n",
    "        t = pm.Data(\"t_obs\", dataset[\"days\"].values, dims=\"obs\")\n",
    "        idx_obs = pm.Data(\"idx_obs\", dataset[\"benchmark_idx\"].values, dims=\"obs\")\n",
    "\n",
    "        # Growth rate\n",
    "        k_mu = pm.Gamma(\n",
    "            \"k_mu\", mu=0.005, sigma=0.002, dims=None if joint else \"benchmark\"\n",
    "        )\n",
    "        k_sigma = pm.HalfNormal(\n",
    "            \"k_sigma\", sigma=0.005, dims=None if joint else \"benchmark\"\n",
    "        )\n",
    "        k = pm.Gamma(\"k\", mu=k_mu, sigma=k_sigma, dims=\"benchmark\")\n",
    "\n",
    "        # Mean latent performance\n",
    "        logits = k[idx_obs] * (t - tau[idx_obs])\n",
    "        if sigmoid_kind == \"logistic\":\n",
    "            sigmoid = pm.math.sigmoid(logits)\n",
    "        elif sigmoid_kind == \"harvey\":\n",
    "            alpha_raw_mu = pm.Gamma(\n",
    "                \"alpha_raw_mu\", mu=1.5, sigma=0.5, dims=None if joint else \"benchmark\"\n",
    "            )\n",
    "            alpha_raw_sigma = pm.HalfNormal(\n",
    "                \"alpha_raw_sigma\", sigma=0.5, dims=None if joint else \"benchmark\"\n",
    "            )\n",
    "            alpha_raw = pm.Gamma(\n",
    "                \"alpha_raw\", mu=alpha_raw_mu, sigma=alpha_raw_sigma, dims=\"benchmark\"\n",
    "            )\n",
    "            alpha = pm.Deterministic(\"alpha\", alpha_raw + 1.0, dims=\"benchmark\")\n",
    "            base = pm.math.maximum(\n",
    "                1 - (1 - alpha[idx_obs]) * pm.math.exp(-logits), 1e-10\n",
    "            )\n",
    "            sigmoid = pm.math.exp(1 / (1 - alpha[idx_obs]) * pm.math.log(base))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported sigmoid type: {sigmoid_kind}\")\n",
    "        mu = pm.Deterministic(\"mu\", l[idx_obs] + (L[idx_obs] - l[idx_obs]) * sigmoid, dims=\"obs\")\n",
    "\n",
    "        # Noise\n",
    "        xi_base_mu = pm.Gamma(\n",
    "            \"xi_base_mu\",\n",
    "            mu=0.05 + top_n / 50,\n",
    "            sigma=0.02,\n",
    "            dims=None if joint else \"benchmark\",\n",
    "        )\n",
    "        xi_base_sigma = pm.HalfNormal(\n",
    "            \"xi_base_sigma\", sigma=0.05, dims=None if joint else \"benchmark\"\n",
    "        )\n",
    "        xi_base = pm.Gamma(\n",
    "            \"xi_base\", mu=xi_base_mu, sigma=xi_base_sigma, dims=\"benchmark\"\n",
    "        )\n",
    "        variance_shape = pm.math.sqrt((mu - l[idx_obs]) * (L[idx_obs] - mu))\n",
    "        max_variance = (L[idx_obs] - l[idx_obs]) / 2.0\n",
    "        noise_factor = variance_shape / pm.math.maximum(max_variance, 1e-10)\n",
    "        xi_0 = 0.01\n",
    "        xi = xi_0 + xi_base[idx_obs] * noise_factor\n",
    "\n",
    "        # Skewness\n",
    "        s_mu = pm.Normal(\n",
    "            \"s_mu\", mu=-2 - top_n / 2, sigma=0.5, dims=None if joint else \"benchmark\"\n",
    "        )\n",
    "        s_sigma = pm.HalfNormal(\"s_sigma\", sigma=1, dims=None if joint else \"benchmark\")\n",
    "        s = pm.TruncatedNormal(\"s\", mu=s_mu, sigma=s_sigma, upper=0, dims=\"benchmark\")\n",
    "\n",
    "        # Observations\n",
    "        y = pm.SkewNormal(\n",
    "            \"y\",\n",
    "            mu=mu,\n",
    "            sigma=xi,\n",
    "            alpha=s[idx_obs],\n",
    "            observed=dataset[\"score\"].values,\n",
    "            dims=\"obs\",\n",
    "        )\n",
    "\n",
    "        # Sample from the posterior\n",
    "        idata = pm.sample(\n",
    "            n_samples,\n",
    "            tune=n_tune,\n",
    "            return_inferencedata=True,\n",
    "            random_seed=42,\n",
    "            target_accept=0.9,\n",
    "            init=\"adapt_diag\",\n",
    "            progressbar=progressbar,\n",
    "        )\n",
    "\n",
    "    return idata, model\n",
    "\n",
    "\n",
    "def _get_frontier(df: pd.DataFrame, top_n: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"Filter the dataset to include only the top_n frontier scores for each benchmark.\n",
    "\n",
    "    Args:\n",
    "        df: A dataset with columns \"benchmark\", \"release_date\", and \"score\".\n",
    "        top_n: Number of top scores to consider when filtering the dataset.\n",
    "    Returns:\n",
    "        A filtered dataset containing only the top_n frontier scores for each benchmark.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.sort_values([\"benchmark\", \"release_date\"])\n",
    "        .assign(\n",
    "            expanding_rank=lambda df: df.groupby(\"benchmark\")[\"score\"]\n",
    "            .expanding()\n",
    "            .rank(ascending=False, method=\"max\")\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        .loc[lambda df: df[\"expanding_rank\"] <= top_n]\n",
    "        .drop(columns=[\"expanding_rank\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5dc747",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_holdout(\n",
    "    dataset: pd.DataFrame,\n",
    "    cutoff_date: pd.Timestamp,\n",
    "    sigmoid_kind: Literal[\"logistic\", \"harvey\"] = \"harvey\",\n",
    "    joint: bool = True,\n",
    "    top_n: int = 3,\n",
    "    progressbar: bool = True,\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"Validate the model by training on data before the cutoff date and testing on data after cutoff date.\n",
    "\n",
    "    Args:\n",
    "        dataset: A dataset containing benchmark data.\n",
    "        cutoff_date: A timestamp to split the dataset into training and testing sets.\n",
    "        sigmoid_kind: Type of sigmoid function to model the latent mean performance growth ('logistic' or 'harvey').\n",
    "        joint: Whether to fit a joint model with shared hyperparameters across benchmarks.\n",
    "        top_n: Top-n frontier scores to consider in the modeling.\n",
    "        progressbar: Whether to display a progress bar during sampling.\n",
    "    Returns:\n",
    "        An arviz InferenceData object containing the posterior samples from the cross-validation in the `predictions` group and true observed values in the `predictions` group.\n",
    "    \"\"\"\n",
    "    dataset = format_dataset_for_modeling(dataset, top_n=top_n)\n",
    "    # Split dataset into training and testing sets\n",
    "    train_dataset = (\n",
    "        dataset.loc[dataset[\"release_date\"] < cutoff_date]\n",
    "        # Fitler out benchmarks with insufficient data at the cutoff date\n",
    "        .loc[\n",
    "            lambda df: df.groupby(\"benchmark\").transform(\"size\")\n",
    "            >= dataset[\"benchmark\"].value_counts().min()\n",
    "        ]\n",
    "    )\n",
    "    test_dataset = dataset[dataset[\"release_date\"] >= cutoff_date][\n",
    "        # Filter out benchmarks not present in training set\n",
    "        dataset[\"benchmark\"].isin(train_dataset[\"benchmark\"].unique())\n",
    "    ]\n",
    "    # Train the model on the training dataset\n",
    "    idata, model = fit_model(\n",
    "        train_dataset,\n",
    "        sigmoid_kind=sigmoid_kind,\n",
    "        joint=joint,\n",
    "        top_n=top_n,\n",
    "        progressbar=progressbar,\n",
    "    )\n",
    "    # Benchmark indices corresponding to those used in the trained_dataset\n",
    "    # Needed for pymc something, unclear what exactly\n",
    "    test_dataset[\"benchmark_idx\"] = pd.Categorical(\n",
    "        test_dataset[\"benchmark\"],\n",
    "        categories=model.coords[\"benchmark\"],\n",
    "        ordered=True,\n",
    "    ).codes\n",
    "    with model:\n",
    "        pm.set_data(\n",
    "            {\n",
    "                \"t_obs\": test_dataset[\"days\"].values,\n",
    "                \"idx_obs\": test_dataset[\"benchmark_idx\"].values,\n",
    "            },\n",
    "            coords={\"obs\": test_dataset.index},\n",
    "        )\n",
    "        idata = pm.sample_posterior_predictive(\n",
    "            idata,\n",
    "            predictions=True,\n",
    "            extend_inferencedata=True,\n",
    "            random_seed=42,\n",
    "            progressbar=progressbar,\n",
    "        )\n",
    "    # Add true observed values to the predictions group for evaluation\n",
    "    idata.predictions[\"y_true\"] = ((\"obs\"), test_dataset[\"score\"].values)\n",
    "    return idata\n",
    "\n",
    "\n",
    "def plot_calibration_curve(idata: az.InferenceData) -> None:\n",
    "    \"\"\"Plot the calibration curve for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        idata: An arviz InferenceData object containing the posterior predictive samples `y` and true observed values `y_true` in the `predictions` group.\n",
    "    \"\"\"\n",
    "    y_pred = idata.predictions.stack(sample=(\"chain\", \"draw\"))[\"y\"].values\n",
    "    y_true = idata.predictions[\"y_true\"].values\n",
    "\n",
    "    confidence_levels = np.linspace(0.01, 0.99, 20)\n",
    "    observed_coverage = []\n",
    "    for p in confidence_levels:\n",
    "        lower_quantile = (1 - p) / 2\n",
    "        upper_quantile = 1 - lower_quantile\n",
    "        lower_bound = np.quantile(y_pred, lower_quantile, axis=1)\n",
    "        upper_bound = np.quantile(y_pred, upper_quantile, axis=1)\n",
    "        is_inside = (y_true >= lower_bound) & (y_true <= upper_bound)\n",
    "        observed_coverage.append(np.mean(is_inside))\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(confidence_levels, observed_coverage, \"o-\", label=\"Model Calibration\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Ideal (Perfectly Calibrated)\")\n",
    "    plt.xlabel(\"Expected Confidence Level (Interval Width)\")\n",
    "    plt.ylabel(\"Observed Coverage (Fraction of Test Data inside Interval)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def crps(idata: az.InferenceData) -> float:\n",
    "    \"\"\"Compute the Continuous Ranked Probability Score (CRPS) for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        idata: An arviz InferenceData object containing the posterior predictive samples `y` and true observed values `y_true` in the `predictions` group.\n",
    "    Returns:\n",
    "        The mean CRPS value across all test data points.\n",
    "    \"\"\"\n",
    "    y_pred = idata.predictions.stack(sample=(\"chain\", \"draw\"))[\"y\"].values\n",
    "    y_true = idata.predictions[\"y_true\"].values\n",
    "\n",
    "    crps = []\n",
    "    for i, (pred, true) in enumerate(zip(y_pred, y_true)):\n",
    "        # `true` is just a number, need to make it an array-like for `energy_distance`\n",
    "        # Squared energy distance divided by 2 equals CRPS\n",
    "        crps.append(energy_distance(pred, (true,)) ** 2 / 2)\n",
    "    return np.mean(crps)\n",
    "\n",
    "\n",
    "def accuracy(idata: az.InferenceData, metric: Literal[\"RMSE\", \"MAE\"] = \"RMSE\") -> float:\n",
    "    \"\"\"Compute the accuracy for the model's predictions.\n",
    "\n",
    "    The accuracy is defined as the RMSE between the expected score and the true observed score for each data point.\n",
    "\n",
    "    Args:\n",
    "        idata: An arviz InferenceData object containing the posterior predictive samples `y` and true observed values `y_true` in the `predictions` group.\n",
    "        metric: The accuracy metric to compute ('RMSE' or 'MAE').\n",
    "    Returns:\n",
    "        The mean accuracy value across all test data points.\n",
    "    \"\"\"\n",
    "    y_pred = idata.predictions.stack(sample=(\"chain\", \"draw\"))[\"y\"].values\n",
    "    y_true = idata.predictions[\"y_true\"].values\n",
    "\n",
    "    accuracies = []\n",
    "    for i, (pred, true) in enumerate(zip(y_pred, y_true)):\n",
    "        expected_score = np.mean(pred)\n",
    "        if metric == \"MAE\":\n",
    "            accuracies.append(abs(expected_score - true))\n",
    "        elif metric == \"RMSE\":\n",
    "            accuracies.append((expected_score - true) ** 2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "\n",
    "    if metric == \"RMSE\":\n",
    "        return np.sqrt(np.mean(accuracies))\n",
    "    elif metric == \"MAE\":\n",
    "        return np.mean(accuracies)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported metric: {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead3e16",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54a635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = pd.to_datetime(\"2025-01-01\")\n",
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec12b6",
   "metadata": {},
   "source": [
    "## Harvey hierarchical (joint) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac816275",
   "metadata": {},
   "outputs": [],
   "source": [
    "harvey_hierarchical_idata = temporal_holdout(\n",
    "    dataset, cutoff_date, sigmoid_kind=\"harvey\", joint=True, top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7999d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Harvey Hierarchical Model CRPS:\", crps(harvey_hierarchical_idata))\n",
    "print(\n",
    "    \"Harvey Hierarchical Model RMSE:\",\n",
    "    accuracy(harvey_hierarchical_idata, metric=\"RMSE\"),\n",
    ")\n",
    "plot_calibration_curve(harvey_hierarchical_idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c062ae0",
   "metadata": {},
   "source": [
    "## Harvey independent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "harvey_independent_idata = temporal_holdout(\n",
    "    dataset, cutoff_date, sigmoid_kind=\"harvey\", joint=False, top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a320fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Harvey Independent Model CRPS:\", crps(harvey_independent_idata))\n",
    "print(\n",
    "    \"Harvey Independent Model RMSE:\", accuracy(harvey_independent_idata, metric=\"RMSE\")\n",
    ")\n",
    "plot_calibration_curve(harvey_independent_idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62493884",
   "metadata": {},
   "source": [
    "## Logistic hierarchical (joint) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_hierarchical_idata = temporal_holdout(\n",
    "    dataset, cutoff_date, sigmoid_kind=\"logistic\", joint=True, top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d81dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Hierarchical Model CRPS:\", crps(logistic_hierarchical_idata))\n",
    "print(\n",
    "    \"Logistic Hierarchical Model RMSE:\",\n",
    "    accuracy(logistic_hierarchical_idata, metric=\"RMSE\"),\n",
    ")\n",
    "plot_calibration_curve(logistic_hierarchical_idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333462a8",
   "metadata": {},
   "source": [
    "## Logistic independent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_independent_idata = temporal_holdout(\n",
    "    dataset, cutoff_date, sigmoid_kind=\"logistic\", joint=False, top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Independent Model CRPS:\", crps(logistic_independent_idata))\n",
    "print(\n",
    "    \"Logistic Independent Model RMSE:\",\n",
    "    accuracy(logistic_independent_idata, metric=\"RMSE\"),\n",
    ")\n",
    "plot_calibration_curve(logistic_independent_idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7951059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all metrics together\n",
    "print(\"Summary of Model Performance:\")\n",
    "models = {\n",
    "    \"Harvey Hierarchical\": harvey_hierarchical_idata,\n",
    "    \"Harvey Independent\": harvey_independent_idata,\n",
    "    \"Logistic Hierarchical\": logistic_hierarchical_idata,\n",
    "    \"Logistic Independent\": logistic_independent_idata,\n",
    "}\n",
    "for model_name, idata in models.items():\n",
    "    model_crps = crps(idata)\n",
    "    model_rmse = accuracy(idata, metric=\"RMSE\")\n",
    "    print(f\"{model_name} - CRPS: {model_crps:.4f}, RMSE: {model_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5442aa6",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b82e7",
   "metadata": {},
   "source": [
    "## Growth rate time evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = format_dataset_for_modeling(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418715af",
   "metadata": {},
   "outputs": [],
   "source": [
    "idata, model = fit_model(df, sigmoid_kind=\"harvey\", joint=True, top_n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_first_dates = df.groupby(\"benchmark\")[\"release_date\"].min().reset_index()\n",
    "\n",
    "inferred_parameters = (\n",
    "    idata.posterior[[\"tau\", \"k\"]]\n",
    "    .mean(dim=(\"chain\", \"draw\"))\n",
    "    .to_dataframe()\n",
    "    .reset_index()\n",
    "    .assign(tau=lambda df: pd.to_timedelta(df[\"tau\"], unit=\"D\"))\n",
    "    .merge(benchmark_first_dates, how=\"left\", on=\"benchmark\")\n",
    "    .assign(tau=lambda df: df[\"release_date\"] + df[\"tau\"])\n",
    ")\n",
    "\n",
    "inferred_parameters.plot.scatter(x=\"tau\", y=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f96cd8",
   "metadata": {},
   "source": [
    "## Test modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca95d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = format_dataset_for_modeling(dataset, top_n=3)\n",
    "idata, model = fit_model(test_data, sigmoid_kind=\"harvey\", joint=False, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    pm.sample_posterior_predictive(idata, extend_inferencedata=True)\n",
    "az.plot_ppc(idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, var_names=[\"L\", \"k\", \"xi_base\", \"s\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654db4cd",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98656491",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset()\n",
    "dataset = format_dataset_for_modeling(dataset, top_n=3)\n",
    "idata, model = fit_model(dataset, sigmoid_kind=\"harvey\", joint=True, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3a306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_benchmark_datapoints(\n",
    "    datasubset: pd.DataFrame, \n",
    "    ax: plt.Axes, \n",
    "    color: str\n",
    ") -> None:\n",
    "    ax.scatter(\n",
    "        datasubset[\"release_date\"],\n",
    "        datasubset[\"score\"],\n",
    "        s=40,\n",
    "        color=color,\n",
    "        marker='o',\n",
    "        edgecolors='white',\n",
    "        alpha=0.4,\n",
    "        zorder=5\n",
    "    )\n",
    "\n",
    "def _create_date_grid(\n",
    "    group: pd.DataFrame,\n",
    "    end_date: pd.Timestamp,\n",
    "    n_points: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create a time grid for a single benchmark group.\"\"\"\n",
    "    start_date = group[\"release_date\"].min()\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, periods=n_points)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"release_date\": date_range, \n",
    "        # Fix: .days works directly on TimedeltaIndex, no .dt needed\n",
    "        \"days\": (date_range - start_date).days,\n",
    "        \"benchmark\": group[\"benchmark\"].iloc[0],\n",
    "        \"category\": group[\"category\"].iloc[0],\n",
    "    })\n",
    "\n",
    "def generate_forecast(\n",
    "    idata: az.InferenceData,\n",
    "    model: pm.Model,\n",
    "    frontier_datapoints: pd.DataFrame,\n",
    "    end_date: pd.Timestamp,\n",
    "    n_points: int = 100,\n",
    "    ci_level: float = 0.8,\n",
    ") -> pd.DataFrame:\n",
    "    # Fix: Use 'frontier_datapoints' argument, not global 'dataset'\n",
    "    predictions = (\n",
    "        frontier_datapoints.groupby(\"benchmark\", group_keys=False)\n",
    "        .apply(\n",
    "            lambda group: _create_date_grid(\n",
    "                group,\n",
    "                end_date=end_date,\n",
    "                n_points=n_points,\n",
    "            )\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # explicit categorical coding to match model coordinates\n",
    "    predictions[\"benchmark_idx\"] = pd.Categorical(\n",
    "        predictions[\"benchmark\"],\n",
    "        categories=model.coords[\"benchmark\"],\n",
    "        ordered=True,\n",
    "    ).codes\n",
    "\n",
    "    with model:\n",
    "        pm.set_data(\n",
    "            {\n",
    "                \"t_obs\": predictions[\"days\"].values,\n",
    "                \"idx_obs\": predictions[\"benchmark_idx\"].values,\n",
    "            },\n",
    "            coords={\"obs\": predictions.index},\n",
    "        )\n",
    "        posterior_predictive = pm.sample_posterior_predictive(\n",
    "            idata,\n",
    "            var_names=[\"mu\"],\n",
    "            predictions=True,\n",
    "            random_seed=42,\n",
    "            progressbar=False,\n",
    "        )\n",
    "    \n",
    "    mu_pred = posterior_predictive.predictions.stack(sample=(\"chain\", \"draw\"))[\"mu\"].values\n",
    "    predictions[\"mu_mean\"] = np.mean(mu_pred, axis=1)\n",
    "    predictions[\"mu_lower\"] = np.quantile(mu_pred, (1 - ci_level) / 2, axis=1)\n",
    "    predictions[\"mu_upper\"] = np.quantile(mu_pred, 1 - (1 - ci_level) / 2, axis=1)\n",
    "\n",
    "    # Process Tau mapping\n",
    "    tau_offset_days = (\n",
    "        idata.posterior[\"tau\"]\n",
    "        .mean(dim=(\"chain\", \"draw\"))\n",
    "        .to_series()\n",
    "        .reset_index(name=\"tau_offset_days\")\n",
    "    )\n",
    "    \n",
    "    start_dates = (\n",
    "        frontier_datapoints.groupby(\"benchmark\", as_index=False)[\"release_date\"]\n",
    "        .min()\n",
    "        .rename(columns={\"release_date\": \"benchmark_first_date\"})\n",
    "    )\n",
    "    \n",
    "    benchmark_metadata = (\n",
    "        pd.merge(start_dates, tau_offset_days, on=\"benchmark\", how=\"inner\")\n",
    "        .assign(\n",
    "            # Fix: Use the correct column name 'benchmark_first_date'\n",
    "            tau_date=lambda df: df[\"benchmark_first_date\"] + pd.to_timedelta(df[\"tau_offset_days\"], unit=\"D\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    predictions = pd.merge(\n",
    "        predictions, benchmark_metadata[[\"benchmark\", \"tau_date\"]],\n",
    "        on=\"benchmark\", how=\"left\"\n",
    "    )\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica']\n",
    "plt.rcParams['axes.facecolor'] = 'none'\n",
    "plt.rcParams['figure.facecolor'] = 'none'\n",
    "plt.rcParams['grid.alpha'] = 0.2\n",
    "\n",
    "colors = {\n",
    "    \"base_color\": \"#0e294c\",\n",
    "    \"accent_color\": \"#d4af37\",\n",
    "    \"gray_color\": \"#6c757d\",\n",
    "    \"grid_color\": \"#5F86A5\",\n",
    "    \"plot_colors\": [\n",
    "        \"#1f4788\", \"#4a7c59\", \"#457b9d\", \"#8b7f7b\", \"#264653\",\n",
    "        \"#6a4c93\", \"#e76f51\", \"#06aed5\", \"#f4a261\", \"#2a9d8f\",\n",
    "    ],\n",
    "}\n",
    "end_date = pd.to_datetime(\"2030-03-01\")\n",
    "label_fontsize = 13\n",
    "\n",
    "# 1. Sample everything once\n",
    "all_forecasts = generate_forecast(\n",
    "    idata, \n",
    "    model, \n",
    "    dataset, \n",
    "    end_date, \n",
    "    n_points=200, \n",
    "    ci_level=0.8\n",
    ")\n",
    "\n",
    "# 2. Plotting Loop\n",
    "for category in dataset[\"category\"].unique():\n",
    "    subset_category = dataset[dataset[\"category\"] == category]\n",
    "    subset_forecast = all_forecasts[all_forecasts[\"category\"] == category]\n",
    "    \n",
    "    # Sort benchmarks by their final predicted value so legend matches visual order\n",
    "    final_perfs = (\n",
    "        subset_forecast[subset_forecast[\"release_date\"] == subset_forecast[\"release_date\"].max()]\n",
    "        .set_index(\"benchmark\")[\"mu_mean\"]\n",
    "    )\n",
    "    sorted_benchmarks = final_perfs.sort_values(ascending=False).index.tolist()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    \n",
    "    for benchmark, color in zip(sorted_benchmarks, itertools.cycle(colors[\"plot_colors\"])):\n",
    "        # Data subsets\n",
    "        bench_obs = subset_category[subset_category[\"benchmark\"] == benchmark]\n",
    "        bench_pred = subset_forecast[subset_forecast[\"benchmark\"] == benchmark]\n",
    "        \n",
    "        # Plot observations\n",
    "        plot_benchmark_datapoints(bench_obs, ax=ax, color=color)\n",
    "        \n",
    "        # Plot forecast\n",
    "        # Split line style at the last observation date\n",
    "        last_obs_date = bench_obs[\"release_date\"].max()\n",
    "        split_idx = bench_pred[\"release_date\"].searchsorted(last_obs_date)\n",
    "        \n",
    "        # Solid line (history)\n",
    "        ax.plot(\n",
    "            bench_pred[\"release_date\"][:split_idx+1],\n",
    "            bench_pred[\"mu_mean\"][:split_idx+1],\n",
    "            linewidth=1.5,\n",
    "            color=color,\n",
    "            alpha=0.8,\n",
    "            label=benchmark,\n",
    "            zorder=6,\n",
    "        )\n",
    "        \n",
    "        # Dashed line (future)\n",
    "        ax.plot(\n",
    "            bench_pred[\"release_date\"][split_idx:],\n",
    "            bench_pred[\"mu_mean\"][split_idx:],\n",
    "            linewidth=1.5,\n",
    "            color=color,\n",
    "            alpha=0.5,\n",
    "            linestyle=(5, (4, 2)),\n",
    "            zorder=6,\n",
    "        )\n",
    "        \n",
    "        # Uncertainty band\n",
    "        ax.fill_between(\n",
    "            bench_pred[\"release_date\"],\n",
    "            bench_pred[\"mu_lower\"],\n",
    "            bench_pred[\"mu_upper\"],\n",
    "            color=color,\n",
    "            alpha=0.15,\n",
    "            linewidth=0,\n",
    "            zorder=4\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(right=end_date)\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "    ax.set_xlabel(\n",
    "        \"\", fontsize=label_fontsize, fontweight=\"500\", color=colors[\"base_color\"]\n",
    "    )\n",
    "\n",
    "    ax.set_ylim(-0.02, 1.05)\n",
    "    ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: \"{:.0%}\".format(y)))\n",
    "    ax.set_ylabel(\n",
    "        \"Performance\",\n",
    "        fontsize=label_fontsize,\n",
    "        fontweight=\"500\",\n",
    "        color=colors[\"base_color\"],\n",
    "    )\n",
    "\n",
    "    ax.grid(True, alpha=0.1, linewidth=0.8, color=colors[\"grid_color\"])\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_color(colors[\"base_color\"])\n",
    "    ax.tick_params(axis=\"y\", left=False, right=False)\n",
    "    ax.tick_params(labelsize=8, colors=colors[\"base_color\"])\n",
    "\n",
    "    legend = ax.legend(\n",
    "        loc=\"lower right\",\n",
    "        fontsize=10,\n",
    "        framealpha=0.95,\n",
    "        edgecolor=colors[\"base_color\"],\n",
    "        fancybox=True,\n",
    "        ncol=1,\n",
    "        handlelength=1.5,\n",
    "    )\n",
    "    for text in legend.get_texts():\n",
    "        text.set_color(colors[\"base_color\"])\n",
    "    for line in legend.get_lines():\n",
    "        line.set_linewidth(2.0)\n",
    "        line.set_linestyle(\"-\")\n",
    "        line.set_alpha(1.0)\n",
    "\n",
    "    ax.set_title(\n",
    "        category, fontsize=13, fontweight=\"600\", color=colors[\"base_color\"], pad=10\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07d5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
